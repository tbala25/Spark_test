{"paragraphs":[{"text":"// Read temperature data for 1986 and station data\n\nvar temps = spark.read.csv(\"/Users/Tejas/Documents/NEU/Spring 2018/DS4300 LSP/hw5/temperatures/1986.csv\").toDF(\"STATION\", \"WBAN\", \"MONTH\", \"DAY\", \"TEMP\")\nvar stations = spark.read.csv(\"/Users/Tejas/Documents/NEU/Spring 2018/DS4300 LSP/hw5/temperatures/stations.csv\").toDF(\"STATION\", \"WBAN\", \"LAT\", \"LON\")","user":"anonymous","dateUpdated":"2018-03-31T21:23:13-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temps: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\nstations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1522439171959_-216233408","id":"20180318-191454_284618957","dateCreated":"2018-03-30T15:46:11-0400","dateStarted":"2018-03-31T21:23:13-0400","dateFinished":"2018-03-31T21:23:13-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:15357"},{"text":"// Filter bad Stations where the latitude or longitude is unavailable\n// Only include temperature data for JANUARY 28th, 1986\n// You'll want to convert latitudes and longitudes to Doubles\nstations = stations.filter(\"LAT is not null\").filter(\"LON is not null\").filter(\"WBAN is not null\").filter(\"STATION is not null\")\n//stations.filter(\"LON is null\").show\n\ntemps = temps.filter($\"MONTH\"=== 1).filter($\"DAY\" === 28)\nstations = stations.withColumn(\"LAT_\", stations(\"LAT\").cast(\"Double\"))\n    .drop(\"LAT\").withColumnRenamed(\"LAT_\", \"LAT\")\nstations = stations.withColumn(\"LON_\", stations(\"LON\").cast(\"Double\"))\n    .drop(\"LON\").withColumnRenamed(\"LON_\", \"LON\")\n    \n//stations.printSchema","user":"anonymous","dateUpdated":"2018-03-31T21:23:14-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"stations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\ntemps: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\nstations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\nstations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1522439171967_-219311399","id":"20180318-191924_1310538952","dateCreated":"2018-03-30T15:46:11-0400","dateStarted":"2018-03-31T21:23:14-0400","dateFinished":"2018-03-31T21:23:15-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15358"},{"text":"// Define a function that compute the distance between two points on the Earth using the Haversine formula\n// https://www.movable-type.co.uk/scripts/latlong.html\n// Declare the function as a UDF (User-defined function) so that it can be applied\n\nimport math._\n\nval pi = 3.14159265\nval REarth = 6371.0 // kilometers\ndef toRadians(x: Double): Double = x * pi / 180.0\n\ndef haversine(lat1: Double, lon1: Double, lat2: Double, lon2: Double): Double = {\n    val rLat = toRadians(lat2 - lat1)\n    val rLon = toRadians(lon2 - lon1)\n    \n    val d_P = pow(sin(rLat/2),2) + pow(sin(rLon/2),2) * cos(lat1.toRadians) * cos(lat2.toRadians)\n    val d = 2 * asin(sqrt(d_P))\n    REarth * d\n}\n\n\n// Now you can use \"haver\" as a function with Spark SQL \nimport org.apache.spark.sql.functions.udf\nval haver = udf(haversine _)","user":"anonymous","dateUpdated":"2018-03-31T21:23:17-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import math._\npi: Double = 3.14159265\nREarth: Double = 6371.0\ntoRadians: (x: Double)Double\nhaversine: (lat1: Double, lon1: Double, lat2: Double, lon2: Double)Double\nimport org.apache.spark.sql.functions.udf\nhaver: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function4>,DoubleType,Some(List(DoubleType, DoubleType, DoubleType, DoubleType)))\n"}]},"apps":[],"jobName":"paragraph_1522439171973_-136975134","id":"20180318-195342_1996330025","dateCreated":"2018-03-30T15:46:11-0400","dateStarted":"2018-03-31T21:23:17-0400","dateFinished":"2018-03-31T21:23:19-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15359"},{"text":"// Find all stations within 100 km using your haversine function\nimport org.apache.spark.sql.functions.udf\n\nval capeCanaveralLatitude = 28.388382\nval capeCanaveralLongitude = -80.603498\n//stations.withColumn(\"cc_LAT\", capeCanaveralLatitude )\n//stations.show\n\n\n\nstations = stations.withColumn(\"haversine\", haver(stations(\"LAT\"), stations(\"LON\"), lit(capeCanaveralLatitude), lit(capeCanaveralLongitude)))\nval cc_100 = stations.filter($\"haversine\" <= 100)\n\n\n//stations.rdd.collect().foreach(x =>println(haversine(x.getDouble(2),x.getDouble(3),capeCanaveralLatitude, capeCanaveralLongitude)))\n\n","user":"anonymous","dateUpdated":"2018-03-31T21:23:23-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions.udf\ncapeCanaveralLatitude: Double = 28.388382\ncapeCanaveralLongitude: Double = -80.603498\nstations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\ncc_100: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATION: string, WBAN: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1522439171980_-139668376","id":"20180318-191955_1910278437","dateCreated":"2018-03-30T15:46:11-0400","dateStarted":"2018-03-31T21:23:23-0400","dateFinished":"2018-03-31T21:23:24-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15360"},{"text":"// Use inverse distance weighting to estimate the temperature at Cape Canaveral on that day\n// You might do this in serveral steps.  First compute a weight for each station within 100 km that recorded\n// a temperature.  Then, when you have a column of weights, apply an aggregation function to\n// multiply each station temperature by a weight and to compute the sum of the weights.\n// This link explains more on inverse distance weighting:\n// https://en.wikipedia.org/wiki/Inverse_distance_weighting\n// Use p=2 in the formula\n\ndef invW(x: Double): Double = {\n    (100 - x)\n}\nval inv = udf(invW _)\n\nval cc100 = cc_100.withColumn(\"weight\", inv(cc_100(\"haversine\")))\n\n//cc100.groupBy(\"STATION\").agg(sum(\"weight\")).show\n\n\nval joined = cc100.join(temps,\"STATION\")\n\njoined.show\n\n\n\n\n\n","user":"anonymous","dateUpdated":"2018-03-31T21:33:31-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"invW: (x: Double)Double\ninv: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(DoubleType)))\ncc100: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 4 more fields]\njoined: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 8 more fields]\norg.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.ThreadUtils$.awaitResultInForkJoinSafely(ThreadUtils.scala:215)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:131)\n  at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:231)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:124)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:123)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n  at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)\n  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:88)\n  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:209)\n  at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n  at org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:141)\n  at org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:333)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:141)\n  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:128)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:88)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:313)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:354)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:225)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:308)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:636)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:595)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:604)\n  ... 60 elided\nCaused by: org.apache.spark.SparkException: Job 105 cancelled part of cancelled job group zeppelin-20180319-091159_1767904139\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1625)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:78)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:75)\n  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:94)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:74)\n  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:74)\n  at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n  at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1522439171987_-128510658","id":"20180319-091159_1767904139","dateCreated":"2018-03-30T15:46:11-0400","dateStarted":"2018-03-31T21:32:51-0400","dateFinished":"2018-03-31T21:33:16-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:15361"},{"text":"// Once you have the weighted sum of temperatures (numerator) and the sum of the weights (denominator)\n// you can obtain your final result\n\n\ndef weightTemp(x:Double, y:Double): Double = {\n    x * y\n}\n\nval wT = udf(weightTemp _)\nvar wTdf = joined.withColumn(\"wT\", joined(\"weight\") * joined(\"TEMP\"))\n//wTdf.show\n\nwTdf.agg(sum(\"wT\")).show\n\nval wtsum =  wTdf.agg(sum(\"wT\")).first.getDouble(0)\nval weightsum = wTdf.agg(sum(\"weight\")).first.getDouble(0)\n\nprintln(wtsum/weightsum)\n\n\n\n\n","dateUpdated":"2018-03-31T22:06:12-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"weightTemp: (x: Double, y: Double)Double\nwT: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,DoubleType,Some(List(DoubleType, DoubleType)))\nwTdf: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 9 more fields]\n+-----------------+\n|          sum(wT)|\n+-----------------+\n|11050.58135124082|\n+-----------------+\n\nwtsum: Double = 11050.58135124082\nweightsum: Double = 318.7066187002251\n34.673209474934\n"}]},"apps":[],"jobName":"paragraph_1522439171993_-132358147","id":"20180318-194257_301533474","dateCreated":"2018-03-30T15:46:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15362","user":"anonymous","dateFinished":"2018-03-31T22:09:02-0400","dateStarted":"2018-03-31T22:06:12-0400"},{"text":"// Extra credit - find average temperature for Jan 28 for every year.\n// Generate a line plot.\n// Was the Jan 28, 1986 temperature unusual?\njoined.show","dateUpdated":"2018-03-31T22:09:59-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1522439171995_-131588649","id":"20180318-194403_1746372836","dateCreated":"2018-03-30T15:46:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15363","user":"anonymous","dateFinished":"2018-03-31T22:10:03-0400","dateStarted":"2018-03-31T22:09:59-0400","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-----+------+-------+------------------+------------------+-----+-----+---+----+\n|STATION| WBAN|   LAT|    LON|         haversine|            weight| WBAN|MONTH|DAY|TEMP|\n+-------+-----+------+-------+------------------+------------------+-----+-----+---+----+\n| 722040|12838|28.101|-80.644| 32.20076129813371| 67.79923870186629|12838|   01| 28|33.7|\n| 722045|12843|27.653|-80.243| 89.09902774108495|10.900972258915047|12843|   01| 28|37.5|\n| 722046|12898|28.517|  -80.8|23.949767660289595|  76.0502323397104| null|   01| 28|37.0|\n| 722050|12815|28.434|-81.325| 70.74640307515355| 29.25359692484645|12815|   01| 28|34.7|\n| 722051|12841|28.545|-81.333| 73.40503810011543|26.594961899884566|12841|   01| 28|15.3|\n| 722056|12834|29.183|-81.048| 98.40471410301785|1.5952858969821477|12834|   01| 28|31.8|\n| 722057|12854| 28.78|-81.244| 76.20658612504654| 23.79341387495346|12854|   01| 28|33.4|\n| 747950|12867|28.233|  -80.6|17.281083196933235| 82.71891680306676|12867|   01| 28|39.6|\n+-------+-----+------+-------+------------------+------------------+-----+-----+---+----+\n\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1522548599821_-222017707","id":"20180331-220959_1450440952","dateCreated":"2018-03-31T22:09:59-0400","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:17170"}],"name":"DS4300_The_Challenger_Accident","id":"2DAW5METP","angularObjects":{"2D9UAJB4H:shared_process":[],"2DBRN493T:shared_process":[],"2D9MEMM9K:shared_process":[],"2DBJBYAD5:shared_process":[],"2DBU1Q2EQ:shared_process":[],"2DA93N6U9:shared_process":[],"2DB2E6WD3:shared_process":[],"2D9HBGEFQ:shared_process":[],"2DBV6DJUF:shared_process":[],"2DASHSA6M:shared_process":[],"2D9X23XE1:shared_process":[],"2D9VSJM6H:shared_process":[],"2DB5EDEJQ:shared_process":[],"2D8KJQRZT:shared_process":[],"2D9DKXY2W:shared_process":[],"2DA3ESU3Z:shared_process":[],"2DBMCB124:shared_process":[],"2D9C7N4EF:shared_process":[],"2DC4JQM3J:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}